{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f09bf16a-5ac5-4ab0-9396-7d0c21eeb300",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) Chatbot with PyPDFLoader\n",
    "\n",
    "This notebook demonstrates a RAG pipeline for answering questions using the contents of a PDF file. It includes:\n",
    "1. **Loading the PDF with PyPDFLoader.**\n",
    "2. **Splitting text into chunks and populating ChromaDB.**\n",
    "3. **Configuring a chatbot with embedding and generative models.**\n",
    "4. **Querying the chatbot with technical questions.**\n",
    "\n",
    "### Key Features:\n",
    "- Simplified PDF loading with `PyPDFLoader`.\n",
    "- Free, open-source embedding and generative models.\n",
    "- Modular and reusable code structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09698dfd-1872-46ad-8a95-620cf3413510",
   "metadata": {},
   "source": [
    "# Load and proccess PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d68e167-a92f-4996-a609-b2ac554c28ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 241 pages from the PDF.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "import torch\n",
    "\n",
    "# Load the PDF using PyPDFLoader\n",
    "pdf_path = \"../data/pdf/pythonlearn.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "\n",
    "# Load and split the document into pages\n",
    "pages = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(pages)} pages from the PDF.\")\n",
    "\n",
    "# Unir páginas con saltos de línea\n",
    "#pdf_text = \"\\n\\n\".join([page.page_content for page in pages])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a012e8-6ab8-4db6-bea5-df1013fe3f78",
   "metadata": {},
   "source": [
    "# Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8187e7aa-f51b-4f1b-ba3e-014e869efe18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import re\\n\\ndef clean_text(text):\\n    # Reemplazar múltiples saltos de línea por uno solo\\n    text = re.sub(r\\'\\n{2,}\\', \\'\\n\\n\\', text)\\n    # Reemplazar múltiples espacios por uno solo\\n    text = re.sub(r\\'[ ]{2,}\\', \\' \\', text)\\n    # Eliminar caracteres no imprimibles\\n    text = re.sub(r\\'[^ -~\\n]\\', \\'\\', text)\\n    return text.strip()\\n\\n# Aplicar la limpieza al texto del PDF\\ncleaned_pdf_text = clean_text(pdf_text)\\n\\nprint(\"Text cleaned correctly!\")\\n\\nprint(cleaned_pdf_text[:1000])'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Reemplazar múltiples saltos de línea por uno solo\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text)\n",
    "    # Reemplazar múltiples espacios por uno solo\n",
    "    text = re.sub(r'[ ]{2,}', ' ', text)\n",
    "    # Eliminar caracteres no imprimibles\n",
    "    text = re.sub(r'[^\\x20-\\x7E\\n]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Aplicar la limpieza al texto del PDF\n",
    "cleaned_pdf_text = clean_text(pdf_text)\n",
    "\n",
    "print(\"Text cleaned correctly!\")\n",
    "\n",
    "print(cleaned_pdf_text[:1000])\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d814fa8c-a47c-4223-8660-31a807a070f1",
   "metadata": {},
   "source": [
    "# Create chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6769f6a2-818e-4b15-b783-862150fa8696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 241 chunks.\n",
      "Chunks generated correctly!\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Configura el divisor de texto\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=10000,  # Tamaño de cada fragmento\n",
    "    chunk_overlap=0,  # Superposición entre fragmentos\n",
    "    #separator=\".\"  # Respetar los saltos de línea\n",
    ")\n",
    "\n",
    "# Dividir el texto en fragmentos\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "print(f\"Created {len(chunks)} chunks.\")\n",
    "\n",
    "print(\"Chunks generated correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db06c3d-4490-494b-a411-0564ddd64e2f",
   "metadata": {},
   "source": [
    "# Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18bcf482-0132-4956-a755-047221e2e08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bootcamp\\AppData\\Local\\Temp\\ipykernel_15172\\2240911563.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "C:\\Users\\Bootcamp\\anaconda3\\envs\\RAG\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB populated and persisted with HuggingFace embeddings.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Crear objetos Document\n",
    "#documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "\n",
    "# Initialize HuggingFace embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize ChromaDB\n",
    "db = Chroma.from_documents(chunks, embedding_model, persist_directory=\"../data/chroma_db\")\n",
    "\n",
    "print(\"ChromaDB populated and persisted with HuggingFace embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6899b2-e41a-40a3-a41c-c80d49555f4f",
   "metadata": {},
   "source": [
    "# ChatBot config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c864de48-0977-40b6-896f-9392dc4963cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load the generative model (e.g., Flan-T5)\n",
    "model_name = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "generative_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(\"cuda\")\n",
    "\n",
    "# Define the chatbot class\n",
    "class GenerativeChatbot:\n",
    "    def __init__(self, db, generative_model, tokenizer):\n",
    "        self.db = db\n",
    "        self.generative_model = generative_model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def retrieve_context(self, user_question, k=5):\n",
    "        results = self.db.similarity_search(user_question, k=k)\n",
    "        return results\n",
    "\n",
    "    def format_context(self, docs):\n",
    "        prompt = \"\\n\"\n",
    "        for doc in docs:\n",
    "            #clean_content = clean_text(doc.page_content)\n",
    "            prompt += f\"Content:\\n{doc.page_content}\\n\\n\"  # Añade saltos claros entre fragmentos\n",
    "        return prompt\n",
    "\n",
    "        \n",
    "    \"\"\"def clean_response(self, response):\n",
    "        # Reemplazar múltiples saltos de línea por uno solo\n",
    "        response = re.sub(r'\\n{2,}', '\\n\\n', response)\n",
    "        # Reemplazar múltiples espacios por uno solo\n",
    "        response = re.sub(r'[ ]{2,}', ' ', response)\n",
    "        # Eliminar caracteres no imprimibles\n",
    "        response = re.sub(r'[^\\x20-\\x7E\\n]', '', response)\n",
    "        return response.strip()\"\"\"\n",
    "        \n",
    "    def generate_response(self, user_question, formatted_context):\n",
    "        # Generate a tailored prompt for Python content\n",
    "        prompt = f\"\"\"\n",
    "        You are a Python programming assistant.\n",
    "    \n",
    "        ## USER QUESTION:\n",
    "        {user_question}\n",
    "    \n",
    "        ## CONTEXT:\n",
    "        The following content has been retrieved from Python programming resources:\n",
    "        '''\n",
    "        {formatted_context}\n",
    "        '''\n",
    "    \n",
    "        ## TASK:\n",
    "        1. Use the CONTEXT provided to answer the user's question directly.\n",
    "        2. Include Python code examples if applicable, using proper formatting (```python ... ```).\n",
    "        3. If the CONTEXT does not contain the answer, respond with: \"The provided context does not contain this information.\"\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        inputs = {key: value.to(\"cuda\") for key, value in inputs.items()}\n",
    "        outputs = self.generative_model.generate(inputs[\"input_ids\"], max_length=500, num_beams=4, early_stopping=True)\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        #response = self.clean_response(response)\n",
    "        \n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81f405c-a35e-4c94-82b2-091bca774fd0",
   "metadata": {},
   "source": [
    "# ChatBot query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9a49b28-b85c-4466-8898-2c56af0af653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def clean_context(docs):\n",
    "    cleaned_docs = []\n",
    "    for doc in docs:\n",
    "        # Si es un objeto Document\n",
    "        if isinstance(doc, Document):\n",
    "            clean_text = re.sub(r'/quotesingle.ts1', \"'\", doc.page_content)  # Reemplazar caracteres extraños\n",
    "            clean_text = re.sub(r'[^\\x20-\\x7E]', '', clean_text)  # Eliminar caracteres no imprimibles\n",
    "            clean_text = re.sub(r'\\s+', ' ', clean_text).strip()  # Normalizar espacios\n",
    "            cleaned_docs.append(Document(page_content=clean_text, metadata=doc.metadata))\n",
    "        # Si es una cadena de texto\n",
    "        elif isinstance(doc, str):\n",
    "            clean_text = re.sub(r'/quotesingle.ts1', \"'\", doc)  # Reemplazar caracteres extraños\n",
    "            clean_text = re.sub(r'[^\\x20-\\x7E]', '', clean_text)  # Eliminar caracteres no imprimibles\n",
    "            clean_text = re.sub(r'\\s+', ' ', clean_text).strip()  # Normalizar espacios\n",
    "            cleaned_docs.append(clean_text)\n",
    "    return cleaned_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49286ef0-9456-4278-b6ab-0e643fc39fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      " ## USER QUESTIONS: How I define a dictionary in Python? ## CONTEXT: The content has been retrieved from Python programming resources: ## CONTEXT:\n"
     ]
    }
   ],
   "source": [
    "# Instanciar el chatbot con los parámetros necesarios\n",
    "chatbot = GenerativeChatbot(db=db, generative_model=generative_model, tokenizer=tokenizer)\n",
    "\n",
    "# Pregunta del usuario\n",
    "user_question = \"How I define a dictionary in Python?\"\n",
    "\n",
    "# Recuperar documentos relevantes\n",
    "retrieved_docs = db.similarity_search(user_question, k=5)\n",
    "\n",
    "# Limpiar los documentos\n",
    "cleaned_docs = clean_context(retrieved_docs)\n",
    "\n",
    "# Formatear el contexto\n",
    "formatted_context = chatbot.format_context(cleaned_docs)\n",
    "\n",
    "# Generar la respuesta\n",
    "response = chatbot.generate_response(user_question, formatted_context)\n",
    "\n",
    "# Mostrar la respuesta\n",
    "print(\"Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c86d74-23a0-47e5-908c-efa29aeb2f14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
