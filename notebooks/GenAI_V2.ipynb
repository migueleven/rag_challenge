{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8268297-36ca-4d88-9257-5ff08fbadd3a",
   "metadata": {},
   "source": [
    "# Text Processing and Question-Answering Workflow\n",
    "\n",
    "This notebook outlines the process of loading a PDF, splitting its content, generating embeddings, building a FAISS index, and configuring a local language model for question-answering.\n",
    "\n",
    "---\n",
    "\n",
    "## Steps\n",
    "\n",
    "### 1. Load and Split Text Data\n",
    "- A PDF document is loaded using the `PyPDFLoader` from LangChain.\n",
    "- The content is split into pages.\n",
    "\n",
    "### 2. Process Text into Fragments\n",
    "- The text is divided into smaller chunks using `RecursiveCharacterTextSplitter`.\n",
    "- This ensures each chunk has a manageable size (e.g., 1000 characters) with an overlap for context preservation.\n",
    "\n",
    "### 3. Convert Chunks into Document Objects\n",
    "- Each text chunk is encapsulated as a `Document` object for further processing.\n",
    "\n",
    "### 4. Generate Embeddings\n",
    "- Embeddings for the text chunks are generated using the `sentence-transformers/all-MiniLM-L6-v2` model.\n",
    "- These embeddings are vector representations of the text, useful for similarity search.\n",
    "\n",
    "### 5. Build a FAISS Index\n",
    "- A FAISS index is created from the document embeddings, enabling efficient similarity-based retrieval.\n",
    "- The index is saved locally for reuse.\n",
    "\n",
    "### 6. Configure a Retriever\n",
    "- The FAISS index is converted into a retriever.\n",
    "- This retriever is configured to return the top 5 most similar results based on a query.\n",
    "\n",
    "### 7. Load and Configure a Local Language Model\n",
    "- The `google/flan-t5-large` model is downloaded and loaded onto a GPU (if available).\n",
    "- A `text2text-generation` pipeline is set up with constraints on input and output lengths.\n",
    "\n",
    "### 8. Create a Question-Answering Chain\n",
    "- A `RetrievalQA` chain is built, combining the retriever and the local language model.\n",
    "- This chain retrieves relevant documents and generates answers based on the query.\n",
    "\n",
    "### 9. Test with a Query\n",
    "- A sample question (e.g., \"How can I create a function in Python?\") is passed to the QA chain.\n",
    "- The chain retrieves relevant information and generates a well-formatted answer.\n",
    "\n",
    "---\n",
    "\n",
    "## Outputs\n",
    "- Number of pages loaded from the PDF.\n",
    "- Number of text fragments generated.\n",
    "- Confirmation of successful index creation and retrieval configuration.\n",
    "- A clean, formatted response to the test query.\n",
    "\n",
    "---\n",
    "\n",
    "This workflow demonstrates how to integrate LangChain components and Hugging Face models to create a functional text-processing and question-answering system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5ed08f-1c43-42d6-aa74-8af1d3492e1c",
   "metadata": {},
   "source": [
    "# 1. Load and Split Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1993568-1923-4bef-ba0e-6ae18932f84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 241 pages from the PDF.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Path to the PDF file\n",
    "pdf_path = \"../data/pdf/pythonlearn.pdf\"\n",
    "\n",
    "# Load the PDF file\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "\n",
    "# Load the PDF content and split it into individual pages\n",
    "pages = loader.load()\n",
    "\n",
    "# Print the number of pages successfully loaded from the PDF\n",
    "print(f\"Loaded {len(pages)} pages from the PDF.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8f4565-c64f-47f4-900d-ce51fc751164",
   "metadata": {},
   "source": [
    "# 2. Process Text into Fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cc0ddb7-8f17-4c25-9a57-82e06f9bec4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 611 fragments.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Configuring the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \"],  # Hierarchical separators\n",
    "    chunk_size=1000,  # Maximum size of each text chunk\n",
    "    chunk_overlap=200  # Overlap between chunks for context retention\n",
    ")\n",
    "\n",
    "# Splitting the content into fragments\n",
    "documents = []\n",
    "for page in pages:\n",
    "    fragments = text_splitter.split_text(page.page_content)\n",
    "    documents.extend(fragments)\n",
    "\n",
    "print(f\"Generated {len(documents)} fragments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3705fcd8-84ec-43b0-a384-8503a9a36b2b",
   "metadata": {},
   "source": [
    "# 3. Convert Chunks into Document Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2646676-1363-4049-8975-8c4032fe83f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 611 document objects.\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Convert fragments into Document objects\n",
    "doc_objects = [Document(page_content=fragment) for fragment in documents]\n",
    "\n",
    "# Print the number of Document objects created\n",
    "print(f\"Created {len(doc_objects)} document objects.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9a8688-3cfa-48cf-af87-b94a629d3ebe",
   "metadata": {},
   "source": [
    "# 4. Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff960347-cd81-4679-8abf-be8bd5c339bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bootcamp\\anaconda3\\envs\\RAG\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Load the embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Generate embeddings directly from the document contents\n",
    "embeddings = embedding_model.embed_documents([doc.page_content for doc in doc_objects])\n",
    "\n",
    "# Print a success message once embeddings are generated\n",
    "print(\"Embeddings generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6736c126-2b22-4b46-a937-1edbc5c9885f",
   "metadata": {},
   "source": [
    "# 5. Build a FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca1d6b1e-0428-4fd2-b618-51d365aa8810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index created and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Create a FAISS index from the Document objects and embeddings\n",
    "vectorstore = FAISS.from_documents(doc_objects, embedding_model)\n",
    "\n",
    "# Save the FAISS index to a local directory\n",
    "vectorstore.save_local(\"../data/index/python_for_everybody\")\n",
    "\n",
    "# Print a success message once the index is created and saved\n",
    "print(\"Index created and saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef2022d-7180-4c0d-a19a-8c584911d450",
   "metadata": {},
   "source": [
    "# 6. Configure retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c5b67be-bbac-4740-bf51-e7bc91a14469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever configured successfully.\n"
     ]
    }
   ],
   "source": [
    "# Configure the retriever from the FAISS index\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",  # Use similarity-based search\n",
    "    search_kwargs={\"k\": 5}  # Retrieve the top 5 most similar results\n",
    ")\n",
    "\n",
    "# Print a success message once the retriever is configured\n",
    "print(\"Retriever configured successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9c808f-7415-4476-887e-3ca02740a709",
   "metadata": {},
   "source": [
    "# 7. Load and Configure a Local Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9362cf8d-001b-45f4-a1a3-5d8a2d2cfaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Download the model and tokenizer\n",
    "model_name = \"google/flan-t5-large\"  # Change to \"base\" or \"xl\" depending on the desired size\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  # Load the tokenizer for the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(\"cuda\")  # Load the model and move it to GPU if available\n",
    "\n",
    "# Print a success message once the model and tokenizer are loaded\n",
    "print(\"Model and tokenizer loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a22eb3-467c-4c85-acba-fcf9ee73ab2f",
   "metadata": {},
   "source": [
    "# 8. Create a Question-Answering Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd543148-ad31-47df-9583-554a0f6a93f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local LLM configured successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# Configure the text2text-generation pipeline with length limits\n",
    "llm_pipeline = pipeline(\n",
    "    \"text2text-generation\",  # Specify the task type\n",
    "    model=model,  # Use the preloaded model\n",
    "    tokenizer=tokenizer,  # Use the preloaded tokenizer\n",
    "    max_length=512,  # Input length limit\n",
    "    max_new_tokens=200,  # Maximum output length\n",
    "    device=0  # Use GPU if available\n",
    ")\n",
    "\n",
    "# Integrate the pipeline with LangChain\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
    "\n",
    "# Print a success message once the local LLM is configured\n",
    "print(\"Local LLM configured successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c03bcc-7916-40fe-9fba-250079bdd46b",
   "metadata": {},
   "source": [
    "# 9. Test with a Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a4f9dce-3408-4e16-8349-bc3bb8c2341d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qa_chain saved successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'# Test the chain with a query\\nquery = \"How can I create a function in Python? Give me an example\"\\n\\n# Generate the response using the QA chain\\nresponse = qa_chain.invoke({\"query\": query})\\nanswer = response[\"result\"]\\n\\n# Clean up the generated text for better readability\\nclean_answer = answer.replace(\"/quotesingle.ts1\", \"\\'\").replace(\": \", \":\\n\\t\").replace(\") \", \")\\n\\t\").strip()\\n\\n# Print the final, cleaned answer\\nprint(f\"Answer:\\n {clean_answer}\")'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create the question-answering chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,  # Configured LLM for text generation\n",
    "    retriever=retriever,  # Configured retriever for document retrieval\n",
    "    chain_type=\"stuff\"  # Strategy for combining retrieved documents\n",
    ")\n",
    "\n",
    "# Ruta donde quieres guardar el archivo\n",
    "save_path = \"../data/model/\"\n",
    "os.makedirs(save_path, exist_ok=True)  # Crear la carpeta si no existe\n",
    "\n",
    "# Save qa_chain file\n",
    "with open(\"../data/model/qa_chain.pkl\", \"wb\") as f:\n",
    "    pickle.dump(qa_chain, f)\n",
    "\n",
    "print(\"qa_chain saved successfully.\")\n",
    "\n",
    "\"\"\"# Test the chain with a query\n",
    "query = \"How can I create a function in Python? Give me an example\"\n",
    "\n",
    "# Generate the response using the QA chain\n",
    "response = qa_chain.invoke({\"query\": query})\n",
    "answer = response[\"result\"]\n",
    "\n",
    "# Clean up the generated text for better readability\n",
    "clean_answer = answer.replace(\"/quotesingle.ts1\", \"'\").replace(\": \", \":\\n\\t\").replace(\") \", \")\\n\\t\").strip()\n",
    "\n",
    "# Print the final, cleaned answer\n",
    "print(f\"Answer:\\n {clean_answer}\")\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
